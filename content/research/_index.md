---
title: Research
summary: My research
type: landing

cascade:
  - _target:
      kind: page
    params:
      show_breadcrumb: true

sections:
  - block: markdown-wide
    content:
      title:
      subtitle: ''
      text: |-
        My primary research interests lie in the areas of natural language processing (NLP) and machine learning (ML). The ultimate goal is to build general intelligent machines that can understand, interact, and help human beings on a wide variety of tasks with trustworthiness and efficiency. I am glad to see LLMs are bringing us one step closer ðŸ¤”

        {{< spoiler text="In the pre-LLM era, I was a trained NLP researcher with background and expertise in..." >}}
        <p style="background-color: lightgray;">
        In the pre-LLM era, I was a trained NLP researcher with background and expertise in, for example, generative models with Transformers, decoding algorithms and trustworthy generation with different resources. I have worked on a wide range of core NLP tasks and methodologies such as sequence to sequence techniques for text generations (automatic summarization, neural machine translation, etc.) and sequence to graph generations (semantic parsing, program generation, etc.) for semantic understanding. My thesis was on <a href="https://dash.harvard.edu/handle/1/37375860">Generating Semantic Graphs for Natural Language</a>.
        </p>
        {{< /spoiler >}}

        Nowadays, my research centers around broad language applications and generative AI, where language includes natural language but can also be considered as any sequential information such as code, serialized visual representations, embodied actions, etc. I am motivated to better understand and improve state-of-the-art deep learning models such as (large) language models (LLMs) and multimodal models, on a variety of aspects such as efficiency, knowledge representation and memorization, factualness, AI safety, fair evaluation, reasoning and planning. Some of the topics could be:
        - Retrieval augmentation
        - Knowledge manipulation
        - Reliable generation
        - Code, structured generation, neuro-symbolic AI
        - Long context
        - Information compression
        - Complex reasoning
        - Inference efficiency
        - Multimodal AI (w/ generation, evaluation, training, and novel applications)
        - Etc.

        #### Recent Interests

        **LLM + RL/reasoning**: I have been thinking a lot about LLM + RL, where RL may unlock new capabilities of LLMs or new ways of generation. There is still much to explore - I maintain an <a href="https://github.com/jzhou316/Post-DeepSeek-R1_LLM-RL">online project</a> of the fast progress in the space after DeepSeek R1 (with my personal bias).

        Please reach out to collaborate ðŸ˜ƒ
        I would also love to collaborate on data-driven interdisciplinary applications, such as applying core NLP/ML techniques in many problems such as in science, sociology, and engineering.
    
    design:
      # css_style: "background-color: #f0f0f0; padding: 10px;"
      # css_class: "markdown-wide"        # this does not seem to work
      columns: '1'
      spacing:
        # Customize the section spacing. Order is top, right, bottom, left.
        padding: ['20px', '0', '0px', '0']
---
