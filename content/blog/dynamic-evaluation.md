
Many benchmarking papers

Benefits:

- reduce data contamination

- decentralize benchmark construction

- keep pace with evolving agents; benchmarks should also self-evolve


[(2024 Mar) LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code](https://livecodebench.github.io/)

[(2024 June) Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models](https://arxiv.org/pdf/2402.11894)

[(2024 June) LiveBench: A Challenging, Contamination-Limited LLM Benchmark](https://livebench.ai/#/)

[(2024 Dec) AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge](https://arxiv.org/abs/2412.13670)

[(2025 Aug) DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033?)

https://futurex-ai.github.io/
